Timeline
Week 1: Focus on learning the basics of NLP and start preparing your dataset.
Week 2: Dive into machine translation models, focusing on Seq2Seq and transformers.
Week 3: Train your model and fine-tune it on your dataset. Evaluate and optimize your model.
Week 4: Develop the app (backend and frontend) and integrate the translation model. Test and deploy your app.


1. Basic to nlp 
2.nlp concepts : Tokenization - Embeddings - Sequence Modeling - Attention Mechanisms
3.familiarization with libraries : NLTK , spacy and transformers
4.Nlp's tasks : Language Models - Seq2Seq Models -  Transformer Models


1. : nlp = branch of AI that allows the computer understand human's language
PIPLINE : text => segmentation(by punctuations)  => tokenization (word by word ) => stemming(root of word ( without pref-suffix)) => lemmatizaion (root of word based on context not just removing (pred_suff)) => part of speech tagging POS :(type of word = grammatical category ) => named id recognition (semantic word)  => parsed text



2.
A.####Tokenization###:
splitt the sentence into word and
sentences =['I love my dog','I love my cat']
tokenizer=Tokenizer(num_words=100)
tokenizer.fit_on_texts(sentences)
word_index=tokenizer.word_index 
print(word_index) : {'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}

##sequencing : turn the sentence into an array of numbers using the tokenizer : sequences = tokenizer.texts_to_sequences(sentences)

##resizing : to have same lenth for each word : we use pad_sentence(sentences) : it adds zeros before by default
##stops word : (is,the,of..) doesn't provide any helpful informations ===> remove them in order to make the task more focused 
either by definig an array of elemnts or using nltk.download('stepwords')

###BAG OF WORDS###:(word vector)
extracting features from text for use in modeling, it describes the occurrence of words within a document.
represent text data numerically : word_index for example (Unique numbers) 

###IF-TF##

#####WORD EMBEDDING####": better than BoW and one hot encoding (zero/one vectors)
is a representation of a word by vectors(real numbers) based on the meaning and relationship between them
similar words => similar vectors 
The Embedding layer is used for word embedding. It converts the integer-encoded sequences from the Tokenizer into dense, continuous-valued vectors (embeddings). Word embeddings capture semantic relationships between words
     #"WORD2VEC"#:
is a technique for word embedding = continuous BoW or skip gram(predict context from target)
it is learnt during NN training : the weight of the word = weight vector (representation)



B###SEQUENCE MODELING ###:

 ##LSTM##:(3gates : in , out, forget)
solution for short-term memory in rnn, we store the important word into long-term-memory and take it the whole the hidden layer while training to predict , when a new important word is presented we throw the ancient and store the new one (forget gate rule)
one hidden state for long

 ##GRU##:gated recurrent unit (2gates)
long and short term memory ==> one hidden state
 the gate have two : reset(how much to forget) and update(how much to retain) 
same equations but diff weight

C###ATTENTION MEHCANISM##":
it is technique sued to improve model's performance by focusing on relevant data from input and assigny diff important weight to elements = attention weight that are calculated based on the similarity or relationship between elmnts
*allows models to process inputs of variable lengths by attending to different parts of the input sequence dynamically.
*specially introduced to improve the encoder-decoder model for translation task , The idea behind the attention mechanism was to permit the decoder to utilize the most relevant parts of the input sequence in a flexible manner, by a weighted combination of all the encoded input vectors, with the most relevant vectors being attributed the highest weights. 


3.A seq2seq model :
it is for sequential data, took a sequence => process it => return a sequence . Its architecture contain two elmnt : 
*ENCODER : process each element of the input using nn or transformer 
process = save internal and ultimate hidden state as a context vector(summary) that catch the semantic meaning and importance info of the input.

*DECODER : process the context vector => generate output sequence incrementally
process = take the context vector and  target, at each time step, the decoder uses the current hidden state, the context vector, and the previous output token to generate a probability distribution over the possible next tokens. It generates one token each step.


B.Transformers:
is a model that uses self-attention that transforms one whole sentence into a single sentence. 
**ARCHITECTURE** : 
Encoder [
*positional encoding: each token has a specific position in the cos and cosin graph , the first token is the first line and so on.
https://www.youtube.22com/watch?v=zxQyTK8quyY : each input = unique sequence of position values => word embedding + positional encoding
======>allow transformer to keep track of word order (incase same words but diff order)

*self attention : 
===>allow to track relationship between words 
works by seeing how similar word to all the others in a sentence, including itself
process : 1.after calculating the positional encoding + word embedding => multiply it by the weight ==> new values = query 
          2.calculate relationship with itself = key : used to calculate similarity for a token by usin DOT PRODUCT(query, key) [either itself and others]
        3:values: create another values to represent the token => scale them => add them => self attention fpot thta token
*reuse same weight - parallel computing - new values contaib input from all of the other words 
        4: take self-att values + positional encoding ===> input encoded!!!!!
]

Decoder [
1. start decoding with <EOS> token (the end of the encoder) => values for <EOS> => calculate the self-att 
2.Allow decoder to keep track of the input in the encoder 
3. we calculate dot product(query <EOS>, encoder's key) => determine what should be the first translated word
]

 









